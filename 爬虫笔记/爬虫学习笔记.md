# 爬虫笔记

## 0 前戏：

1. 你是否在夜深人静的时候，想看一些会让你更睡不着的图片却苦于没有资源...

2. 你是否在节假日出行高峰的时候，想快速抢购火车票成功...
3. 你是否在网上购物的时候，想快速且精准的定位到口碑质量最好的商品...

### 什么是爬虫：

    - 通过编写程序，模拟浏览器上网，然后让其去互联网上抓取数据的过程。

### 爬虫的价值：

- 实际应用

    - 就业

### 爬虫究竟是合法还是违法的？

- 在法律中是不被禁止
- 具有违法风险
- 善意爬虫  恶意爬虫

#### 爬虫带来的风险可以体现在如下2方面：

   - 爬虫干扰了被访问网站的正常运营
   - 爬虫抓取了收到法律保护的特定类型的数据或信息

#### 如何在使用编写爬虫的过程中避免进入局子的厄运呢？

- 时常的优化自己的程序，避免干扰被访问网站的正常运行
- 在使用，传播爬取到的数据时，审查抓取到的内容，如果发现了涉及到用户隐私
        商业机密等敏感内容需要及时停止爬取或传播

### 爬虫在使用场景中的分类

   -  通用爬虫：
             抓取系统重要组成部分。抓取的是一整张页面数据。
   - 聚焦爬虫：
     是建立在通用爬虫的基础之上。抓取的是页面中特定的局部内容。
   - 增量式爬虫：
     检测网站中数据更新的情况。只会抓取网站中最新更新出来的数据。

### 爬虫的矛与盾

- 反爬机制
        门户网站，可以通过制定相应的策略或者技术手段，防止爬虫程序进行网站数据的爬取。

- 反反爬策略
        爬虫程序可以通过制定相关的策略或者技术手段，破解门户网站中具备的反爬机制，从而可以获取门户网站中相关的数据。

- robots.txt协议：
        君子协议。规定了网站中哪些数据可以被爬虫爬取哪些数据不可以被爬取。



http协议
   - 概念：就是服务器和客户端进行数据交互的一种形式。

    常用请求头信息
     - User-Agent：请求载体的身份标识
     - Connection：请求完毕后，是断开连接还是保持连接

  常用响应头信息

   - Content-Type：服务器响应回客户端的数据类型

https协议：

- 安全的超文本传输协议 

加密方式
   - 对称秘钥加密

   - 非对称秘钥加密

   - 证书秘钥加密

        

## 1 requests模块

   - urllib模块
   - requests模块

requests模块：python中原生的一款基于网络请求的模块，功能非常强大，简单便捷，效率极高。
作用：模拟浏览器发请求。

- 如何使用：（requests模块的编码流程）

    - 指定url
    - UA伪装
    - 请求参数的处理
    - 发起请求
    - 获取响应数据
    - 持久化存储

- 环境安装：
        pip install requests

- 实战编码：
        需求：爬取搜狗首页的页面数据



### 实战巩固

 需求：爬取搜狗指定词条对应的搜索结果页面（简易网页采集器）

- UA检测

- UA伪装

    

需求：破解百度翻译

- post请求（携带了参数）
- 响应数据是一组json数据



爬取豆瓣电影分类排行榜 https://movie.douban.com/中的电影详情数据

    - 作业：爬取肯德基餐厅查询http://www.kfc.com.cn/kfccda/index.aspx中指定地点的餐厅数据
    
    - 需求：爬取国家药品监督管理总局中基于中华人民共和国化妆品生产许可证相关数据
                http://125.35.6.84:81/xk/
        - 动态加载数据
        - 首页中对应的企业信息数据是通过ajax动态请求到的。
    
        http://125.35.6.84:81/xk/itownet/portal/dzpz.jsp?id=e6c1aa332b274282b04659a6ea30430a
        http://125.35.6.84:81/xk/itownet/portal/dzpz.jsp?id=f63f61fe04684c46a016a45eac8754fe
        - 通过对详情页url的观察发现：
            - url的域名都是一样的，只有携带的参数（id）不一样
            - id值可以从首页对应的ajax请求到的json串中获取
            - 域名和id值拼接处一个完整的企业对应的详情页的url
        - 详情页的企业详情数据也是动态加载出来的
            - http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsById
            - http://125.35.6.84:81/xk/itownet/portalAction.do?method=getXkzsById
            - 观察后发现：
                - 所有的post请求的url都是一样的，只有参数id值是不同。
                - 如果我们可以批量获取多家企业的id后，就可以将id和url形成一个完整的详情页对应详情数据的ajax请求的url

### 数据解析：

-  聚焦爬虫
- 正则
-  bs4
-  xpath



### 1. 聚焦爬虫

爬取页面中指定的页面内容。
    - 编码流程：
        - 指定url
        - 发起请求
        - 获取响应数据
        - 数据解析
        - 持久化存储

数据解析分类：

- 正则
- bs4
    - xpath（***）

数据解析原理概述：
    - 解析的局部的文本内容都会在标签之间或者标签对应的属性中进行存储
        - 1.进行指定标签的定位
        - 2.标签或者标签对应的属性中存储的数据值进行提取（解析）

### 2. 正则解析



```html
<div class="thumb">
<a href="/article/121721100" target="_blank">
<img src="//pic.qiushibaike.com/system/pictures/12172/121721100/medium/DNXDX9TZ8SDU6OK2.jpg" alt="指引我有前进的方向">
</a>
</div>
```

ex = '<div class="thumb">.*?<img src="(.*?)" alt.*?</div>'

### 3. bs4进行数据解析

```
- 数据解析的原理：

- 1.标签定位
- 2.提取标签、标签属性中存储的数据值

   - bs4数据解析的原理：

        - 实例化一个BeautifulSoup对象，并且将页面源码数据加载到该对象中

        - 2.通过调用BeautifulSoup对象中相关的属性或者方法进行标签定位和数据提取

          

   - 环境安装：

        - pip install bs4
          pip install lxml

   - 如何实例化BeautifulSoup对象：
     from bs4 import BeautifulSoup

     

     对象的实例化：

     - 1.将本地的html文档中的数据加载到该对象中
             fp = open('./test.html','r',encoding='utf-8')
             soup = BeautifulSoup(fp,'lxml')
     - 2.将互联网上获取的页面源码加载到该对象中
             page_text = response.text
             soup = BeatifulSoup(page_text,'lxml')

     提供的用于数据解析的方法和属性：

     - soup.tagName:返回的是文档中第一次出现的tagName对应的标签
     - soup.find():
         - find('tagName'):等同于soup.div
         - 属性定位：
             - soup.find('div',class_/id/attr='song')
     - soup.find_all('tagName'):返回符合要求的所有标签（列表）
            - select：
     - select('某种选择器（id，class，标签...选择器）'),返回的是一个列表。
     - 层级选择器：
         - soup.select('.tang > ul > li > a')：>表示的是一个层级
         - oup.select('.tang > ul a')：空格表示的多个层级
             - 获取标签之间的文本数据：
     - soup.a.text/string/get_text()
     - text/get_text():可以获取某一个标签中所有的文本内容
     - string：只可以获取该标签下面直系的文本内容
            - 获取标签中属性值：
     - soup.a['href']




```



### 4. xpath解析

最常用且最便捷高效的一种解析方式。通用性。

    - xpath解析原理：
        - 1.实例化一个etree的对象，且需要将被解析的页面源码数据加载到该对象中。
        - 2.调用etree对象中的xpath方法结合着xpath表达式实现标签的定位和内容的捕获。
    - 环境的安装：
        - pip install lxml
    - 如何实例化一个etree对象:from lxml import etree
        - 1.将本地的html文档中的源码数据加载到etree对象中：
            etree.parse(filePath)
        - 2.可以将从互联网上获取的源码数据加载到该对象中
            etree.HTML('page_text')
        - xpath('xpath表达式')
    - xpath表达式:
        - /:表示的是从根节点开始定位。表示的是一个层级。
        - //:表示的是多个层级。可以表示从任意位置开始定位。
        - 属性定位：//div[@class='song'] tag[@attrName="attrValue"]
        - 索引定位：//div[@class="song"]/p[3] 索引是从1开始的。
        - 取文本：
            - /text() 获取的是标签中直系的文本内容
            - //text() 标签中非直系的文本内容（所有的文本内容）
        - 取属性：
            /@attrName     ==>img/src



## 2.验证码识别



验证码和爬虫之间的爱恨情仇？
		反爬机制：验证码.识别验证码图片中的数据，用于模拟登陆操作。



### 识别验证码的操作：

   - 人工肉眼识别。（不推荐）
- 第三方自动识别（推荐）
     - 云打码：http://www.yundama.com/demo.html
     - 云打码的使用流程：
         - 注册：普通和开发者用户
         - 登录：
           - 普通用户的登录：查询该用户是否还有剩余的题分
           - 开发者用户的登录：
               - 创建一个软件：我的软件-》添加新软件-》录入软件名称-》提交（软件id和秘钥）
               - 下载示例代码：开发文档-》点此下载：云打码接口DLL-》PythonHTTP示例下载

     实战：识别古诗文网登录页面中的验证码。
     使用打码平台识别验证码的编码流程：

     -  将验证码图片进行本地下载
     -  调用平台提供的示例代码进行图片数据识别







## 3.模拟登录

```tex
python爬虫模拟登录是什么意思

网站需要登录后才能所需要的信息，此时可以设计爬虫进拟登录，原理是利用浏cookie。

一、浏览器访问服务器的过程：

(1)浏览器(客户端)向Web服务器发出一个HTTP请求(Http request)；

(2)Web服务器收到请求，发回响应信息(Http Response)；

(3)浏览器解析内容呈现给用户。


Http请求消息：

(1)起始行：包括请求方法、请求的资源、HTTP协议的版本号

这里GET请求没有消息主体，因此消息头后的空白行中没有其他数据。

(2)消息头：包含各种属性

(3)消息头结束后的空白行

(4)可选的消息体：包含数据

Http响应消息：

(1)起始行：包括HTTP协议版本，http状态码和状态

(2)消息头：包含各种属性

(3)消息体：包含数据
```



### 1.cookie

```tex
    - 手动处理：通过抓包工具获取cookie值，将该值封装到headers中。（不建议）
    - 自动处理：
        - cookie值的来源是哪里？
            - 模拟登录post请求后，由服务器端创建。
        session会话对象：
            - 作用：
                1.可以进行请求的发送。
                2.如果请求过程中产生了cookie，则该cookie会被自动存储/携带在该session对象中。
        - 创建一个session对象：session = requests.Session()
        - 使用session对象进行模拟登录post请求的发送（cookie就会被存储在session中）
        - session对象对个人主页对应的get请求进行发送（携带了cookie）
```

### 2. 代理

```tex
什么是代理：
    - 代理服务器。
代理的作用：
    - 突破自身IP访问的限制。
    - 隐藏自身真实IP
代理相关的网站：
    - 快代理
    - 西祠代理
    - www.goubanjia.com
代理ip的类型：
    - http：应用到http协议对应的url中
    - https：应用到https协议对应的url中

代理ip的匿名度：
    - 透明：服务器知道该次请求使用了代理，也知道请求对应的真实ip
    - 匿名：知道使用了代理，不知道真实ip
    - 高匿：不知道使用了代理，更不知道真实的ip
```







## 4.高性能异步爬虫

目的：在爬虫中使用异步实现高性能的数据爬取操作效率较低

```text
异步爬虫的方式：
    - 1.多线程，多进程（不建议）：
        好处：可以为相关阻塞的操作单独开启线程或者进程，阻塞操作就可以异步执行。
        弊端：无法无限制的开启多线程或者多进程。
    - 2.线程池、进程池（适当的使用）：
        好处：我们可以降低系统对进程或者线程创建和销毁的一个频率，从而很好的降低系统的开销。
        弊端：池中线程或进程的数量是有上限。

- 3.单线程+异步协程（推荐）：
    event_loop：事件循环，相当于一个无限循环，我们可以把一些函数注册到这个事件循环上，
    当满足某些条件的时候，函数就会被循环执行。

    coroutine：协程对象，我们可以将协程对象注册到事件循环中，它会被事件循环调用。
    我们可以使用 async 关键字来定义一个方法，这个方法在调用时不会立即被执行，而是返回
    一个协程对象。

    task：任务，它是对协程对象的进一步封装，包含了任务的各个状态。

    future：代表将来执行或还没有执行的任务，实际上和 task 没有本质区别。

    async 定义一个协程.

    await 用来挂起阻塞方法的执行。
```



## 5.selenium 模块的使用



问题： selenium 模块和爬虫之间有怎么样的关系？

-   便捷的获取网站中当他加载的数据
-   便捷的实现模拟登录



### 什么是selenium模块？

-   基于浏览器自动化的一个模块。

https://book.apeland.cn/details/172/

### selenium的使用流程

```
下载一个浏览器的驱动程序 比如说chromedriver
下载路径http://chromedriver.storage.googleapis.com/- 
驱动程序 http://blog.csdn.net/huilan_same/article/details/51896672
```

```
实例化一个浏览器对象
编写基于浏览器自动化的操作代码
```

```
发起请求 get（url）
标签定位 find 系列
标签交互 send_key
执行js程序 excute
前进后退 back forward
推出 quit
```

###  selenium 处理iframe

```
如果定位的标签存在于ifarme中 必须使用 switch_to.frame()
动作链 （拖动） from selenium.webdriver import ActionChains

实例化一个动作链对象 action=ActionChains()
长按点击并操作  click_and_hold(div)
move_by_offset(x,y)
perform()让动作立刻执行
action.release 释放动作链对象
```



### selenium 实现无可视化界面与规避

-   selenium 实现无可视化界面

    -   ```python
        from selenium.webdriver.chrome.options import Options
        # 无可视化界面
        chrome_options=Options()
        chrome_options.add_argument('--headless')
        chrome_options.add_argument('--disable-gpu')
        
        ```



-   selenium 实现浏览器规避

    -   ```python
        from selenium.webdriver import ChromeOptions
        
        # 实现规避操作
        options=ChromeOptions()
        options.add_experimental_option('excludeSwitches',['enable-automation'])
        ```





-   注册到浏览器对象中

    ```python
    bro=webdriver.Chrome('chromedriver.exe',chrome_options=chrome_options,options=options)
    
    ```



-   3.9最新版源码

    ```python
    from selenium import webdriver
    from time import sleep
    from selenium.webdriver.chrome.options import Options
    
    from selenium.webdriver import ChromeOptions
    
    option=ChromeOptions()
    option.add_argument('--headless')
    option.add_argument('--disable-gpu')
    option.add_experimental_option('excludeSwitches',['enable-automation'])
    
    bro=webdriver.Chrome('chromedriver.exe',options=option)
    
    bro.get('https://www.baidu.com')
    
    print(bro.page_source)
    sleep(3)
    bro.quit()
    ```






### 12306模拟登录 



#### 1. 超级鹰

http://www.chaojiying.com/user/

-   注册： 普通用户
-   登录： 普通用户
    -   提分查询
    -   注册软件id
    -   下载软件开发文档 
    -   使用



#### 2.12306模拟操作 、

保证要一一对应

-   使用selenium 打开登录页面
-   对当前selenium打开的这张页面进行截图
-    对当前图片局部区域进行裁剪 也就是验证码图片
    -   好处：将验证码图片和模拟登录进行一一对应
-   使用超级鹰识别验证码图片





## 7. scrapy 框架

### 1. scrapy 初认识

-   什么是框架？
    -   继承了很多共并且有很多通用性的一个项目
-   如何学习框架
    -   专门学习框架封装的各种功能的详细用法
-   什么是scrapy
    -   爬虫中封装的一个明星框架  功能十分强大  
        -   功能：
            -   高性能的持久化存储
            -   异步数据下载
            -   高性能数据解析
            -   分布式



### 2. scrapy 项目

-   创建工程 ：scrapy startproject xxx
-   cd xxx目录中
-   在spiders子目录中创建一个爬虫文件
    -   scrapy genspider spiderName www.baidu.com
    -    
-   执行工程 
    -   scrapy crawl spidername



### 3. scrapy 数据解析



### 4. scrapy 持久化存储

-   基于终端命令
    -   要求：只可以将parse方法的返回值存储到本地的文本文件中
    -   只能用于Json lxml csv等文件
    -   指令： scrapy crawl xxx -o filepath
    -   好处：简洁高效便捷
    -   缺点：局限性强 数据只能存在指定后缀的文本中
-   基于管道
    -   编码流程
        -   数据解析
        -   在item中定义相关的属性
        -   将解析的数据封装存储到item类型的对象中
        -   将item类型的对象提交给管道进行持久化存储的操作 
        -   在管道类中的procee_item 中见他更要接受到item对象中存储的数据进行持久化存储
        -   在配置文件中开启管道
    -   好处：
        -   通用性强。
    -   缺点
        -   操作繁琐
        



-   面试题：将爬取的数据一份存储到本地 一份存储到数据库？ 如何实现

```
import pymysql
from itemadapter import ItemAdapter


class QiubaiproPipeline:
    fp=None
    def open_spider(self,spider):
        print('开始爬虫')
        self.fp=open('./qiubai.txt','w+',encoding='utf-8')

    # 专门用来处理item对象的 操作
    # 该方法可以接受item提交过来的数据
    # 该方法每接受一次item 就要被调用一次
    def process_item(self, item, spider):

        author=item['author']
        content=item['content']
        self.fp.write(author+':'+content)


        return item  # 就会传递给下一个即将被执行的管道类

    def close_spider(self,spider):
        print('结束爬虫')
        self.fp.close()

class MysqlPipeLine:
    conn=None
    cursor=None

    def open_spider(self,spider):
        self.conn=pymysql.Connect(host='192.168.244.142',port=3306,database='qiubai',charset='utf8',user='root',password='admin*123',)


    def process_item(self, item, spider):
        self.cursor = self.conn.cursor()

        try:
            self.cursor.execute('insert into qiubai values ("%s","%s")'%(item["author"],item["content"]))
            self.conn.commit()
        except Exception as e:
            print(e)
            self.conn.rollback()

        return item

    def close_spider(self,spider):
        self.cursor.close()
        self.conn.close()


# 爬虫文件提交的item类型的对象最终会提交给哪一个管道类？
#     - 给的一定是限制性的管道类
```



-   管道文件中的一个管道类对应的将数据存储到一种平台

-   爬虫文件将提交的item只会给管道文件中第一个被执行的管道类接受

-   process_item中的return item表示将item传递给下一个即将被执行的管道类

-   所以我们需要养成良好的习惯 在每个管道中都需要写上return




### 5. 基于spider的全站数据爬取

-   就是将网站中某个板块下的全部页码对应的页面数据进行爬取
-   需求： 爬取笑话网中照片的名称
-   实现方式
    -   将所有的url 添加到urls列表中 （不推荐）
    -   自行手动进行请求发送（推荐）
        -   手动阿松请求
            -   yield scrapy.Request(url,callback) :callback 专门用于数据解析



### 6. 五大核心组件

-   spider 
    -   产生url 对url进行请求发送 
    -   对数据事务进行解析  然后封装对象给引擎
-   引擎
    -   再将请求给调度器
-   调度器
    -   第一个组成部分——过滤器
        -   将引擎调过来的请求对象进行过滤器去重
    -   第二个组成部分——队列
        -   然后 把请求对象给队列

-   下载器
    -   引擎再将对象给下载器
    -   下载器再向互联网下载 

-   管道

    -   下载完数据 会返回一个response、返回给引擎
    -   引擎再给parse 再给item item再给引擎 引擎再给 管道

    

引擎 1.可以触发事务

​	2.  可以用作数据流处理  



互联网





### 7. 请求传参

-   使用场景: 如果爬取解析的数据不在同一张页面中就要用到传参（深度爬取）
-   需求： 爬取Boss直聘的岗位名称和岗位描述



### 8. 图片数据爬取

-   ImagePipeline
-   基于scrapy爬取字符串类型的数据和爬取图片类型的数据区别？
    -   字符串：只需要基于xpath进行解析并且提交管道进行持久化存储
    -   图片 xpath解析出src的属性值 单独的对图片地址发起请求获取图片二进制类型的数据
-   ImagePiple：
    -   只需要将img的src 属性值进行解析 提交到管道 管道就会对图片的src 请求发送，并且 获取图片的二进制数据 并且进行持久化存储



需求：

-   使用流程：
    -   https://sc.chinaz.com/
    -   进行数据解析 解析图片地址
    -   将存储图片的地址的item提交给指定的管道类
    -   在管道文件中自定制一个基于ImagesPipeLine的一个管道类
        -   get_media_request()
        -   file_path()
        -   item_completed()
    -   在配置文件中指定目录和开启管道 修改为自定义的管道类



### 9. 中间件

-   下载中间件 
    -   位置 处于引擎和下载器中间
    -   作用： 批量拦截到整个工程中发起的所有的请求和响应
    -   拦截请求：
        -   UA伪装：
        -   代理ip 更换ip
    -   拦截响应：
        -   篡改响应内容，响应对象
        -   需求： 爬取网易新闻的数据（爬取新闻的标题和内容）
            -   通过网易新闻的首页解析出板块的url（没有动态加载）
            -   每一个板块对应的新闻标题都是动态加载出来的
            -   通过解析出每一条新闻详情页的url 获取详情页的页面源码



## 10.CrawlSpider

类：基于spider的一个子类

-   全站数据爬取的方式
    -   基于spider： 通过手动请求发送
-   基于CrawlSpider
    -   创建一个工程
    -   cd XXX
    -   创建爬虫文件（CrawlSpider）：
        -   scrapy genspider -t crawl xxx www.xxx.com
        -   链接提取器：
            -   作用：根据指定规则（allow）进行指定链接的提取
        -   规则解析器：
            -   作用: 将链接提取器提取到的链接进行指定规则（callback）的解析
            -   follow=True 可以将链接提取器继续作用到链接提取器提取的链接所对应的页面中
    -   ：todo： 爬取sun网站中的编号，新闻标题 内容以及编号
        -   分析： 爬取的数据没有在同一页面上
        -   1.   可以使用链接提取器提到的所有页面
            2.   让所有链接提取器提取所有的新闻详情页的链接




## 11. 分布式爬虫

-   概念： 我们需要搭建一个分布式集群 对其对一组资源进行分布联合爬取
-   作用： 提升爬取数据的效率 



-   ​	如何实现分布式

    -   安装 scrapy-redis 的组件

    -   原生的scrapy是不可以实现分布式爬虫 必须要让scrapy 结合这scrapy-redis组件 一起实现分布式爬虫

    -   为什么原生的scrapy 不可以实现分布式？

        -   调度器不可以被分布式集群共享
        -   管道不可以被分布式集群共享

    -   scrapy-redis 组件作用：

        -   可以给原生的scrapy 框架提供可以被共享的管道和调度器

    -   实现流程：

        -   创建一个工程

        -   创建一个基于CrawlSpider的爬虫文件

        -   修改当前的爬虫文件：

            -   导包
            -   将start_urls 和allowed_domains 注释掉
            -   添加一个新属性redis_key='key' 可以被共享的调度器队列的名称
            -   编写数据解析相关的 操作
            -   将当前爬虫类的父类修改成RedisCrawlSpider

        -   修改配置文件

            -   指定使用可以被共享的管道 以及被封装好了
            -   指定管道 指定调度器

            -   ```python
                # 指定好的管道
                ITEM_PIPELINES = {
                    'scrapy_redis.pipelines.RedisPipeline': 300
                }
                
                # 调度器启用Redis存储Requests队列
                SCHEDULER = "scrapy_redis.scheduler.Scheduler"
                
                # 确保所有的爬虫实例使用Redis进行重复过滤
                DUPEFILTER_CLASS = "scrapy_redis.dupefilter.RFPDupeFilter"
                
                # 将Requests队列持久化到Redis，可支持暂停或重启爬虫
                SCHEDULER_PERSIST = True
                ```

        指定redis

        

        -   redis 相关操作配置：
            -   配置redis的配置文件：
                -   linux 或者 mac： redis.conf
                -   windows： redis.windows.conf
                -   打开配置文件
                    -   将redis.conf 127.0.0.1 注释掉
                    -   protected-mode yes 改为no
                -   结合配置文件开启redis 服务
                    -   redis -server 配置文件
                -   启动客户端
                    -   redis-cli
            -   执行工程：
                -   scrapy runspider xxx.py
            -   向调度器的队列中放入一个起始url：
                -   调度器的队列 在redis的客户端中
                    -   lpush xxx www.xxx.com
                -   爬取到的数据存储到了redis的proName：items数据结构中



## 12. 增量式爬虫

-   概念： 检测数据网站更新的情况  指挥爬取网站最新更新出来的数据。

-   todo： 爬取电影名称 和电影简介

-   分析 

    -   指定一个起始的url
    -   基于CrawlSpider 获取其他页码链接
    -   基于rule 将其他页码对应的源码解析出每一个电影详情页的url

    

    -   核心： 检测电影详情页的url 之前有没有请求过
        -   将爬取过的电影详情页的url存储 
            -   存储到redis的set数据结构中
    
    
    
    -   对详情页的url 发起请求 然后解析出电影名称 和简介
    -   进行持久化存储
    
-   smembers urls

